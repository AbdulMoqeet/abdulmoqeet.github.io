<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	h1 {
		font-weight:300;
	}
	.disclaimerbox {
		background-color: #eee;
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}
	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		        15px 15px 0 0px #fff, /* The fourth layer */
		        15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		        20px 20px 0 0px #fff, /* The fifth layer */
		        20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		        25px 25px 0 0px #fff, /* The fifth layer */
		        25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}
	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	.vert-cent {
		position: relative;
	    top: 50%;
	    transform: translateY(-50%);
	}
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
  <head>
		<title>Polarity Loss for Zero-shot Object Detection</title>
		<meta property="og:image" content="https://salman-h-khan.github.io/images/Fig1_PL-ZSD.JPG"/>
		<meta property="og:title" content="Polarity Loss for Zero-shot Object Detection" />
  </head>

  <body>
    <br>
         <center><span style="font-size:42px">Polarity Loss for Zero-shot Object Detection </span> <br><br> 
          <table align=center width=900px>
	<tr>
	<td align=center width=100px>
	<center><span style="font-size:20px"><a href="https://sites.google.com/site/rshafin/home">Shafin Rahman</a></span></center>
	</td>
        
	<td align=center width=100px>
	<center><span style="font-size:20px"><a href="https://salman-h-khan.github.io/">Salman Khan</a></span></center>
	</td>
				
        <td align=center width=100px>
	<center><span style="font-size:20px"><a href="http://users.cecs.anu.edu.au/~nmb/">Nick Barnes</a></span></center></td>	
	</table>
	<br>
		 
	 <table align=center width=500px>
	 <tr>	 
         <td align=center width=350px>
	 <center><span style="font-size:20px">Australian National University, Data61-CSIRO, Inception Institute of AI</span>
	 </center></td>
	  </table>	 

	 <table align=center width=500px>
	 <tr><td align=center width=150px>
	 <center><span style="font-size:20px"><a href='https://github.com/salman-h-khan/PL-ZSD_Release'> [GitHub]</a></span>
	 </center></td>
	 
         <td align=center width=350px>
	 <center><span style="font-size:20px"><a href='https://arxiv.org/pdf/1811.08982.pdf'> [Paper + Supplementary Material]</a></span>
	 </center></td>
	  </table>
          </center>

<!-- <br><br> ------------------------------------------------------------------------------<hr> -->

  		  <br>
  		  <table align=center width=850px>
  			  <tr>
  	              <td width=500px>
  			<center><a href="https://salman-h-khan.github.io/images/Fig1_PL-ZSD.JPG"><img src = "https://salman-h-khan.github.io/images/Fig1_PL-ZSD.JPG" height="300px"></img></href></a><br>
			</center> </td>
                </tr>
  	              <td width=400px>
  		<center> 
		<span style="font-size:14px"><i>(Top Left) Traditional ZSD approaches align visual features (solid dots) to their corresponding semantics
(boat/airplane/bicycle) without considering the related semantic concepts (black text). It results in a fragile description of an unseen class (train) and causes confusion with background and seen
classes (bottom left). (Top Right) Our approach automatically attends to related semantics from an external vocabulary and reshapes the semantic embedding such that visual features are wellaligned with seen word vectors and related semantics. Moreover,
it maximizes the inter-class separation that avoids confusion between unseen and background (bottom right).</i>
		</center>
  	              </td>

  		  </table>

      	  <br>
	<!-- Abstract or short description -->
		  <hr>
		  Zero-shot object detection is an emerging research topic that aims to recognize and localize previously ‘unseen’ objects. 
      This setting gives rise to several unique challenges, e.g., highly imbalanced positive vs. negative instance ratio, 
      ambiguity between background and unseen classes and the proper alignment between visual and semantic concepts.
      Here, we propose an end-to-end deep learning framework underpinned by a novel loss function that puts more emphasis on 
      difficult examples to avoid class imbalance. We call our objective the ‘Polarity loss’ because it explicitly maximizes 
      the gap between positive and negative predictions. Such a margin maximizing formulation is important as it
      improves the visual-semantic alignment while resolving the  ambiguity between background and unseen. Our approach is 
      inspired by the embodiment theories in cognitive science, that claim human semantic understanding to be grounded in
      past experiences (seen objects), related linguistic concepts (word dictionary) and the perception of the physical world 
      (visual imagery). To this end, we learn to attend to a dictionary of related semantic concepts that eventually refines 
      the noisy semantic embeddings and helps establish a better synergy between visual and semantic domains. Our extensive results 
      on MS-COCO and Pascal VOC datasets show as high as 14x mAP improvement over state of the art.
		  <br><br>
		  <hr>

	  <!-- Code -->
 		<center><h1>Code</h1></center>
  		  <table align=center width=1000px>
  			<tr>
  	              <td align=center width=850px>
  			<center>
			<td><a href='https://github.com/salman-h-khan/PL-ZSD_Release'><img class="round" style="width:1050" src="https://salman-h-khan.github.io/images/Fig2_PL-ZSD.JPG"/></a></td>
	  		</center>
	  		  	<!-- </br> -->
			</tr>
		  </table>

  		  <table align=center width=800px>
			  <tr><center> <br>
				<!-- <span style="font-size:28px">Code coming soon!</span></i>			  	 -->
				<span style="font-size:28px">&nbsp;<a href='https://github.com/salman-h-khan/PL-ZSD_Release'>[GitHub]</a>

				<span style="font-size:28px"></a></span>
			  <br>
			  </center></tr>
		  </table>
      	  <br>
		  <hr>

  		  <!-- Paper -->
  		  <table align=center width=550>
	 		<center><h1>Paper</h1></center>
  			  <tr>
  	              <!--<td width=300px align=left>-->
  	              <!-- <a href="http://arxiv.org/pdf/1603.08511.pdf"> -->
				  <td><a href="https://arxiv.org/abs/1803.06049"><img class="layered-paper-big" style="height:175px" src="https://salman-h-khan.github.io/images/Fig4_PL-ZSD.JPG"/></a></td>
				  <td><span style="font-size:14pt">S. Rahman, S. H. Khan, N. Barnes.<br>
						Polarity Loss for Zero-Shot Object Detection.<br>
				  arXiv preprint arXiv:1811.08982, 2019.<br>
				   [<a href="https://arxiv.org/pdf/1811.08982.pdf">Link to Paper</a>]</a>
				  <span style="font-size:4pt"><a href="https://arxiv.org/pdf/1811.08982.pdf"><br></a>
				  </span>
				  </td>
  	              </td>
              </tr>
  		  </table>
		  <br>

		  <!-- <table align=center width=600px>
			  <tr>
				  <td><span style="font-size:14pt"><center>
				  	<a href="./resources/bibtex.txt">[Bibtex]</a>
  	              </center></td>
              </tr>
  		  </table> -->

		  <hr>

			<a name="results"></a>
			<center><h1>Results</h1></center>
			Our framework allows us to detect both seen and unseen object cateogries in natural images. We achieve significant improvement over the state of the art. 

			<br><br><br>
			<table align=center width=1100px>
				<tr height="1050px">
					<td valign="top" width=1000px>
					<center>
					<a href="https://salman-h-khan.github.io/images/Fig3_PL-ZSD.JPG"><img img src = "https://salman-h-khan.github.io/images/Fig3_PL-ZSD.JPG" width = "550px"></a><br>
					<span style="font-size:14px">.</span><br>
					</center>
					</td>

				</tr>
			</table>

	 <br><br>
			<hr>
  		  <table align=center width=1100px>
  			  <tr>
  	              <td width=400px>
  			<left>
	  		<center><h1>Citation</h1></center>
	  		  
			S. Rahman, S. H. Khan and N. Barnes, 
			“Polarity Loss for Zero-shot Object Detection,”
			arXiv preprint arXiv:1811.08982, 2019. 	<br><br>
			
						
			<font face="Courier New">		
      @article{rahman2018polarity,<br>
      title={Polarity Loss for Zero-shot Object Detection},<br>
      author={Rahman, Shafin and Khan, Salman and Barnes, Nick},<br>
      journal={arXiv preprint arXiv:1811.08982},<br>
      year={2018}}
      </font>
						
			</left>
		</td>
			 </tr>
		</table>

		<br><br>
     <!-- Previous Work -->
		<hr>
  		  <table align=center width=1100px>
  			  <tr>
  	              <td width=400px>
  			<left>
	  		<center><h1>Previous Work</h1></center>
	  		  
			This works is a follow up work on our previous contribution: <a href="https://salman-h-khan.github.io/ProjectPages/ZSD_Arxiv18">Zero-shot Detection: Learning to Simultaneously Recognize and Localize Novel Concepts</a>.
						
			</left>
		</td>
			 </tr>
		</table>

		<br><br>
	

     <!-- Acknowledgements -->
		<hr>
  		  <table align=center width=1100px>
  			  <tr>
  	              <td width=400px>
  			<left>
	  		<center><h1>Ancknowledgement</h1></center>
	  		  
			This project page template was modified from <a href="https://richzhang.github.io/colorization/">this webpage</a>.
						
			</left>
		</td>
			 </tr>
		</table>

		<br><br>

		
</body>
</html>
