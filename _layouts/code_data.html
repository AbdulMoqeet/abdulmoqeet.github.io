<!doctype html>
<head>
{% include _head.html %}
<meta name="google-site-verification" content="t0BRFGbT_qpFQ-8bKqRhR2vcKliLttXNwtqBigtUnY0" />
</head>

<body class="page">
<style>
    /*********************************
     The list of publication items
     *********************************/
/* The list of items */
.biblist { }

/* The item */
.biblist li { }

/* You can define custom styles for plstyle field here. */


/*************************************
 The box that contain BibTeX code
 *************************************/
div.noshow { display: none; }
div.bibtex {
	margin-right: 0%;
	margin-top: 1.2em;
	margin-bottom: 1em;
	border: 1px solid silver;
	padding: 0em 1em;
	background: #ffffee;
}
div.bibtex pre { font-size: 75%; overflow: auto;  width: 100%; padding: 0em 0em;}</style>
<script type="text/javascript">
    <!--
    // Toggle Display of BibTeX
    function toggleBibtex(articleid) {
        var bib = document.getElementById('bib_'+articleid);
        if (bib) {
            if(bib.className.indexOf('bibtex') != -1) {
                bib.className.indexOf('noshow') == -1?bib.className = 'bibtex noshow':bib.className = 'bibtex';
            }
        } else {
            return;
        }
    }
-->
    </script>

{% include _browser-upgrade.html %}

{% include _navigation.html %}

{% if page.image.feature %}
  <div class="image-wrap">
  <img src=
    {% if page.image.feature contains 'http://' %}
      "{{ page.image.feature }}"
    {% elsif page.image.feature contains 'https://' %}
      "{{ page.image.feature }}"
    {% else %}
      "{{ site.url }}/images/{{ page.image.feature }}"
    {% endif %}
  alt="{{ page.title }} feature image">
  {% if page.image.credit %}
    <span class="image-credit">Photo Credit: <a href="{{ page.image.creditlink }}">{{ page.image.credit }}</a></span>
  {% endif %}
  </div><!-- /.image-wrap -->
{% endif %}

<div id="main" role="main">
  <div class="article-author-side">
    {% include _author-bio.html %}
  </div>
  <article>
    <h1>{{ page.title }}</h1>
    <div class="article-wrap">
      {{ content }}

<!----------------------------------------------------------------------------------->
<!-- Generated from JabRef by PubList by Truong Nghiem at 12:49 on 2016.07.15. -->


<!-------------------------------------------------------------------------------------------------------->
<h4 style="margin-bottom:0px;padding-top:20px;">Datasets and Protocols</h4>

<ul>
  <li> <b> AnimalWeb - A Large-Scale Hierarchical Dataset of Annotated Animal Faces:</b> 
	  We introduce a largescale, hierarchical annotated dataset of animal faces, featuring 21.9K faces captured ‘in-the-wild’ conditions.
	  These faces belong to 334 diverse species, while covering 21 different animal orders across biological taxonomy. 
	  Each face is consistently annotated with 9 landmarks on key facial features. It is structured and scalable by design; its development underwent four systematic stages involving rigorous, manual annotation effort of over 6K man-hours. We benchmark the proposed dataset for face alignment using the existing art under two new problem settings. 
	  Results showcase its challenging nature, unique attributes and present definite prospects for novel, adaptive, and generalized face-oriented CV algorithms. 
	  We further benchmark the dataset across related tasks, namely face detection and fine-grained recognition, to demonstrate multi-task applications and opportunities for improvement.
	  For more details, please see our <a href="https://salman-h-khan.github.io/papers/CVPR20_AnimalWeb.pdf">paper</a> and <a href="https://fdmaproject.wordpress.com/">dataset page</a>.
	  </li>
	
  <li> <b>iSAID - A Large-scale Dataset for InstanceSegmentation in Aerial Images:</b> Existing Earth Vision datasets are either 
	  suitable for semantic segmentation or object detection. iSAID is the first benchmark dataset for instance segmentation in 
	  aerial images. This large-scale and densely annotated dataset contains 655,451 object instances for 15 categories across 
	  2,806 high-resolution images. The distinctive characteristics of iSAID are the following: (a) large number of images with 
	  high spatial resolution, (b) fifteen important and commonly occurring categories, (c) large number of instances per 
	  category, (d) large count of labelled instances per image, which might help in learning contextual information, (e) huge 
	  object scale variation, containing small, medium and large objects, often within the same image, (f) Imbalanced and uneven 
	  distribution of objects with varying orientation within images, depicting real-life aerial conditions, (g) several small 
	  size objects, with ambiguous appearance, can only be resolved with contextual reasoning, (h) precise instance-level 
	  annotations carried out by professional annotators, cross-checked and validated by expert annotators complying with 
	  well-defined guidelines. For more detail, please refer to our <a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/DOAI/Zamir_iSAID_A_Large-scale_Dataset_for_Instance_Segmentation_in_Aerial_Images_CVPRW_2019_paper.pdf">paper</a> and 
	  the <a href="https://captain-whu.github.io/iSAID/index.html">dataset</a> page.
	</li>
	
  <li><b>ImageNet Zero-Shot Object Detection Protocol:</b> The train/val/test splits for zero-shot object detection based on ILSVRC 
	  object detection dataset are avilable <a href="https://github.com/salman-h-khan/ZSD_Release/tree/master/ImageNet2017">here</a>.
	  The intructions on how to use the proposed splits are available <a href="https://github.com/salman-h-khan/ZSD_Release/tree/master/ImageNet2017">here</a>.
	  The motivation and details for the proposed train and test protocol can be found in the associated <a href="https://arxiv.org/abs/1803.06049">publication</a> 
	  and <a href="https://salman-h-khan.github.io/ProjectPages/ZSD_Arxiv18">project page</a>.
	</li>
  <li><b>MS-COCO Zero-Shot Object Detection Protocol:</b> The train/val/test splits for zero-shot object detection based on MS-COCO
	  object detection dataset are avilable <a href="https://github.com/salman-h-khan/PL-ZSD_Release/tree/master/MSCOCO">here</a>.
	  The intructions on how to use the proposed splits are available <a href="https://github.com/salman-h-khan/PL-ZSD_Release">here</a>.
	  The motivation and details for the proposed train and test protocol can be found in the associated <a href="https://arxiv.org/pdf/1811.08982.pdf">publication</a> 
	  and <a href="https://salman-h-khan.github.io/ProjectPages/ZSD_Arxiv19">project page</a>.
	</li>
  <li><b>Object Categories in Indoor Scenes:</b> This database contains a total of 15,324 images spanning more than 1300 frequently 
	  occurring indoor object categories. The database can potentially be used for fine-grained scene categorization, high-level 
	  scene understanding and attribute-based reasoning. The dataset is available for download <a href="https://ucstaff-my.sharepoint.com/:f:/g/personal/munawar_hayat_canberra_edu_au/EuTYS7f0DeRKgyI0rmIBLj8BvazrhTYpPlVqdB4kIFa5Ug?e=flAPcq">here</a>. 
	  More details about the dataset can be found in the associated <a href="https://salman-h-khan.github.io/papers/TIP16_1.pdf">publication</a>.</li>
</ul>



<!-------------------------------------------------------------------------------------------------------->
<h4 style="margin-bottom:0px;padding-top:10px;">Codes</h4>
<ul>

<li>
<a href="https://salman-h-khan.github.io/papers/CVPR20_CycleISP.pdf">CycleISP: Real Image Restoration via Improved Data Synthesis</a> <a href="https://github.com/swz30/CycleISP">[Code Link]</a> (CVPR'20)
</li>
	
<li>
<a href="https://salman-h-khan.github.io/papers/CVPR20_NRP.pdf">A Self-supervised Approach for Adversarial Robustness</a> <a href="https://github.com/Muzammal-Naseer/NRP">[Code Link]</a> (CVPR'20)
</li>

<li>
<a href="https://salman-h-khan.github.io/papers/CVPR20_iTAML.pdf">iTAML: An Incremental Task-Agnostic Meta-learning Approach</a> <a href="https://github.com/brjathu/iTAML">[Code Link]</a>  (CVPR'20)
</li>
	
<li>
<a href="https://salman-h-khan.github.io/papers/CVPR20_SEMIT.pdf">Semi-supervised Learning for Few-shot Image-to-Image Translation</a> <a href="https://github.com/yaxingwang/SEMIT">[Code Link]</a> (CVPR'20)
</li>
	
<li>
<a href="https://arxiv.org/abs/1905.11736">Cross-Domain Transferability of Adversarial Perturbations</a> <a href="https://github.com/Muzammal-Naseer/Cross-domain-perturbations">[Code Link]</a> (NeurIPS'19)	
</li>

<li>
<a href="https://arxiv.org/pdf/1906.01120.pdf">Random Path Selection for Incremental Learning</a> <a href="https://github.com/brjathu/RPSnet">[Code Link]</a> (NeurIPS'19)
</li>

<li>
<a href="https://arxiv.org/pdf/1906.01308.pdf">Towards better Validity: Dispersion based Clustering for Unsupervised Person Re-identification</a> <a href="https://github.com/gddingcs/Dispersion-based-Clustering">[Code Link]</a> (BMVC'19)
</li>
	
<li>
<a href="https://arxiv.org/abs/1904.00887">Adversarial Defense by Restricting the Hidden Space of Deep Neural Networks</a> <a href="https://github.com/aamir-mustafa/pcl-adversarial-defense">[Code Link]</a> (ICCV'19)
</li>
	
<li>
<a href="https://arxiv.org/abs/1901.01677">Image Super-Resolution as a Defense Against Adversarial Attacks</a> <a href="https://github.com/aamir-mustafa/super-resolution-adversarial-defense">[Code Link]</a> (IEEE TIP'19)
</li>
	
<li>
<a href="https://arxiv.org/abs/1811.08982">Polarity Loss for Zero-shot Detection</a> <a href="https://github.com/salman-h-khan/PL-ZSD_Release">[Code Link]</a>	
</li>
	
<li>
<a href="https://arxiv.org/pdf/1803.06049.pdf">Zero-Shot Object Detection: Learning to Simultaneously Recognize and Localize Novel Concepts</a> <a href="https://github.com/salman-h-khan/ZSD_Release">[Code Link]</a> (ACCV'18)
</li>
	
<li>
<a href="https://salman-h-khan.github.io/papers/IJCV17.pdf">Empowering Simple Binary Classifiers for Image Set based Face Recognition</a> <a href="https://www.google.com/url?q=https%3A%2F%2Fucstaff-my.sharepoint.com%2Fpersonal%2Fmunawar_hayat_canberra_edu_au%2F_layouts%2F15%2Fguestaccess.aspx%3Fdocid%3D04fdc49d7e1f24628bc25a37f86ff5288%26authkey%3DAWiUcfK1lmijf2isTzmjIo4%26e%3D9a02090aa0504085976ca95b201897e7&sa=D&sntz=1&usg=AFQjCNGv-Tkm99WxrCgSBQxD-6w9_Gu2bQ">[Code Link]</a> (IJCV'17)	
</li>
	
<li>
<a href="https://salman-h-khan.github.io/codes/PlnDet_ECCV14.zip">Plane Detection Code</a> for <a href="https://salman-h-khan.github.io/papers/ECCV14.pdf">Geometry Driven Semantic Labeling of Indoor Scenes</a> (ECCV'14)	
</li>
	
</ul>

    </div><!-- /.article-wrap -->
    {% if site.disqus_shortname and page.comments %}
      <section id="disqus_thread"></section><!-- /#disqus_thread -->
    {% endif %}
  </article>
</div><!-- /#index -->
<div class="footer-wrap">
  <footer>
    {% include _footer.html %}
  </footer>
</div><!-- /.footer-wrap -->

{% include _scripts.html %}          

</body>
</html>
